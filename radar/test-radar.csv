name,ring,quadrant,status,isNew,description
Dependency driftingxx fitness function,Adopt,Techniques,no change,TRUE,"<p>Fitness functions introduced by <a href=""/radar/Techniques/evolutionary-architecture"">evolutionary architecture</a>, borrowed from <a href=""https://en.wikipedia.org/wiki/Evolutionary_computation#:%7E:text=In%20computer%20science%2C%20evolutionary%20computation,soft%20computing%20studying%20these%20algorithms."">evolutionary computing</a>, are executable functions that inform us if our applications and architecture are objectively moving away from their desired characteristics. They're essentially tests that can be incorporated into our release pipelines. One of the major characteristics of an application is the freshness of its dependencies to other libraries, APIs or environmental components that a <strong>dependency drift fitness function</strong> tracks to flag the out-of-date dependencies that require updating. With the growing and maturing number of tools that detect dependency drifts, such as <a href=""/radar/tools/dependabot"">Dependabot</a> or <a href=""/radar/tools/snyk"">Snyk</a>, we can easily incorporate dependency drift fitness functions into our software release process to take timely action in keeping our application dependencies up to date.</p>"
Run cost as architecture fitness function,Adopt,Techniques,moved in ,FALSE,"<p>Automating the estimation, tracking and projection of cloud infrastructure's run cost is necessary for today's organizations. The cloud providers' savvy pricing models, combined with the proliferation of pricing parameters and the dynamic nature of today's architecture, can lead to surprisingly expensive run costs. For example, the price of <a href=""/radar/Techniques/serverless-architecture"">serverless</a> based on API calls, event streaming solutions based on traffic or data processing clusters based on running jobs, all have a dynamic nature that changes over time as the architecture evolves. When our teams manage infrastructure on the cloud, implementing <strong>run cost as architecture fitness function</strong> is one of their early activities. This means that our teams can observe the cost of running services against the value delivered; when they see deviations from what was expected or acceptable, they'll discuss whether it's time to evolve the architecture. The observation and calculation of the run cost is implemented as an automated function.</p>"
Security policy as code,Adopt,Techniques,moved out,FALSE,"<p>As the technology landscape is becoming more complex, concerns such as security need more automation and engineering practices. When building systems, we need to take into consideration security policies, which are rules and procedures to protect our systems from threats and disruption. For example, access control policies define and enforce who can access which services and resources under what circumstances; by contrast, network security policies can dynamically limit the traffic rate to a particular service.</p>

<p>Several of our teams have had a great experience treating <strong>security policy as code</strong>. When we say <em>as code</em>, we not only mean to write these security policies in a file but also to apply practices such as keeping the code under version control, introducing automatic validation in the pipeline, automatically deploying them in the environments and observing and monitoring their performance. Based on our experience and the maturity of the existing tools — including <a href=""/radar/tools/open-policy-agent-opa"">Open Policy Agent</a> and platforms such as <a href=""/radar/platforms/istio"">Istio</a> which provide flexible policy definition and enforcement mechanisms that support the practice of security policy as code — we highly recommend using this technique in your environment.</p>"
Tailored service templates,Adopt,Techniques,new,FALSE,"<p>Since we last mentioned <strong>tailored service templates</strong>, we've seen a broader adoption of the pattern to help pave the road for organizations moving to microservices. With constant advances in observability tooling, container orchestration and service mesh sidecars, a template provides sensible defaults to bootstrap a new service, removing a great deal of setup needed to make the service work well with the surrounding infrastructure. We've had success <a href=""/radar/Techniques/applying-product-management-to-internal-platforms"">applying product management</a> principles to tailored service templates, treating internal developers as customers and making it easier for them to push code to production and operate it with appropriate observability. This has the added benefit of acting as a lightweight governance mechanism to centralize default technical decisions.</p>"
Continuous delivery for machine learning (CD4ML),Trial,Techniques,no change,FALSE,"<p>About a decade ago we introduced <a href=""/radar/Techniques/continuous-delivery-cd"">continuous delivery (CD)</a>, our default way to deliver software solutions. Today's solutions increasingly include machine-learning models and we find them no exception in adopting continuous delivery practices. We call this <strong><a href=""https://martinfowler.com/articles/cd4ml.html"">continuous delivery for machine learning (CD4ML)</a></strong>. Although the principles of CD remain the same, the practices and tools to implement the end-to-end process of training, testing, deploying and monitoring models require some modifications. For example: version control must not only include code but also the data, the models and its parameters; the testing pyramid extends to include model bias, fairness and data and feature validation; the deployment process must consider how to promote and evaluate the performance of new models against current champion models. While the industry is celebrating the new buzzword of MLOps, we feel CD4ML is our holistic approach to implement an end-to-end process to reliably release and continuously improve machine-learning models, from idea to production.</p>"
Data mesh,Trial,Techniques,no change,FALSE,"<p><strong><a href=""https://martinfowler.com/articles/data-monolith-to-mesh.html"">Data mesh</a></strong> marks a welcome architectural and organizational paradigm shift in how we manage big analytical data. The paradigm is founded on four principles: (1) domain-oriented decentralization of data ownership and architecture; (2) domain-oriented data served as a product; (3) self-serve data infrastructure as a platform to enable autonomous, domain-oriented data teams; and (4) federated governance to enable ecosystems and interoperability. Although the principles are intuitive and attempt to address many of the known challenges of previous centralized analytical data management, they transcend the available analytical data technologies. After building data mesh for multiple clients on top of the existing tooling, we learned two things: (a) there is a large gap in open-source or commercial tooling to accelerate implementation of data mesh (for example, implementation of a universal access model to time-based polyglot data which we currently custom build for our clients) and (b) despite the gap, it's feasible to use the existing technologies as the basic building blocks.</p>

<p>Naturally, technology fit is a major component of implementing your organization's data strategy based on data mesh. Success, however, demands an organizational restructure to separate the data platform team, create the role of data product owner for each domain and introduce the incentive structures necessary for domains to own and share their analytical data as products.</p>"
Declarative data pipeline definition,Trial,Techniques,no change,FALSE,"<p>Many data pipelines are defined in a large, more or less imperative script written in Python or Scala. The script contains the logic of the individual steps as well as the code chaining the steps together. When faced with a similar situation in Selenium tests, developers discovered the Page Object pattern, and later many behavior-driven development (BDD) frameworks implemented a split between step definitions and their composition. Some teams are now experimenting with bringing the same thinking to data engineering. A separate <strong>declarative data pipeline definition</strong>, maybe written in YAML, contains only the declaration and sequence of steps. It states input and output data sets but refers to scripts if and when more complex logic is needed. <a href=""https://github.com/binaryaffairs/a-la-mode"">A La Mode</a> is a relatively new tool that takes a DSL approach to defining pipelines, but <a href=""https://github.com/rambler-digital-solutions/airflow-declarative"">airflow-declarative</a>, a tool that turns directed acyclic graphs defined in YAML into <a href=""/radar/tools/airflow"">Airflow</a> task schedules, seems to have the most momentum in this space.</p>"
Diagrams as code,Trial,Techniques,no change,TRUE,"<p>We're seeing more and more tools that enable you to create software architecture and other <strong>diagrams as code</strong>. There are benefits to using these tools over the heavier alternatives, including easy version control and the ability to generate the DSLs from many sources. Tools in this space that we like include <a href=""https://diagrams.mingrammer.com/"">Diagrams</a>, <a href=""https://structurizr.com/dsl"">Structurizr DSL</a>, <a href=""https://asciidoctor.org/docs/asciidoctor-diagram/"">AsciiDoctor Diagram</a> and stables such as <a href=""https://www.websequencediagrams.com/"">WebSequenceDiagrams</a>, <a href=""/radar/tools/plantuml"">PlantUML</a> and the venerable <a href=""https://graphviz.org/"">Graphviz</a>. It's also fairly simple to generate your own SVG these days, so don't rule out quickly writing your own tool either. One of our authors wrote a small <a href=""/radar/languages-and-frameworks/ruby"">Ruby</a> script to quickly create SVGs, for example.</p>"
Distroless Docker images,Trial,Techniques,no change,FALSE,"<p>When building <a href=""/radar/platforms/docker"">Docker</a> images for our applications, we're often concerned with two things: the security and the size of the image. Traditionally, we've used <a href=""/radar/Techniques/container-security-scanning"">container security scanning</a> tools to detect and patch <a href=""https://cve.mitre.org/"">common vulnerabilities and exposures</a> and small distributions such as <a href=""https://alpinelinux.org/"">Alpine Linux</a> to address the image size and distribution performance. We've now gained more experience with <strong>distroless Docker images</strong> and are ready to recommend this approach as another important security precaution for containerized applications. Distroless Docker images reduce the footprint and dependencies by doing away with a full operating system distribution. This technique reduces security scan noise and the application attack surface. There are fewer vulnerabilities that need to be patched and as a bonus, these smaller images are more efficient. Google has published a set of <a href=""https://github.com/GoogleContainerTools/distroless"">distroless container images</a> for different languages. You can create distroless application images using the Google build tool <a href=""https://bazel.build/"">Bazel</a> or simply use multistage Dockerfiles. Note that distroless containers by default don't have a shell for debugging. However, you can easily find debug versions of distroless containers online, including a <a href=""https://busybox.net/downloads/BusyBox.html"">BusyBox shell</a>. Distroless Docker images is a technique pioneered by Google and, in our experience, is still largely confined to Google-generated images. We're hoping that the technique catches on beyond this ecosystem.</p>"
